{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# use polynomial function as integrand and normal distribution as probability measure\n",
    "\n",
    "## different type of control variates are implemented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load modules\n",
    "from scipy import spatial\n",
    "#import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "# load modules\n",
    "import torch\n",
    "from scipy import stats\n",
    "#%matplotlib inline\n",
    "from scipy import special\n",
    "import time,pdb,json\n",
    "from copy import copy\n",
    "from collections import defaultdict\n",
    "from torch.autograd import Variable\n",
    "from torch import nn\n",
    "from torch.autograd import grad\n",
    "import torch.nn.functional as F\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "### import control variates from other files\n",
    "from neural_networks_control_variates import SteinFirstOrderNeuralNets\n",
    "from GaussianKernelControlVariates import GaussianControlVariate,OatesGram\n",
    "from polynomial_control_variates import SteinSecondOrderQuadPolyCV\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### integrand:  f(x1, x2,...,x_m) = \\Sum_{i=1}^{m}\\Sum_{j=0}^{n-1}Alpha_{ij}x_i^{j}\n",
    "### normal probability measure: (x1, x2, ..., x_m) \\sim N(0, sigma^2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the integration and the gradient\n",
    "class Prepare_Poly(object):\n",
    "    def __init__(self, Alpha, sigma):\n",
    "        \"\"\"\n",
    "        initialization of integrands\n",
    "        f(x1, x2,...,x_m) = \\Sum_{i=1}^{m}\\Sum_{j=0}^{n-1}Alpha_{ij}x_i^{j}\n",
    "\n",
    "        :params[in], Alpha, two-dimensional torch array, i.e., matrix\n",
    "        :params[in], sigma, the standard deviation of Gaussian distr.\n",
    "        \"\"\"\n",
    "        self.Alpha = Alpha  # matrix of constants in the integrand\n",
    "        self.dim, self.order = Alpha.size() # dimension of x, and the highest order of polynomial\n",
    "        self.sigma = sigma\n",
    "        \n",
    "    def vect_power_matrix(self, x):\n",
    "        \"\"\"\n",
    "        for each input 1-d tensor x, find the matrix of its powers\n",
    "        \n",
    "        :params[in], x, a d-dimensional 1-d tensor\n",
    "        \n",
    "        :params[out], res, a 2-d tensor, each column is x^i\n",
    "        \"\"\"\n",
    "        powers = [x.pow(i) for i in range(self.order)]\n",
    "        res = torch.stack(powers, dim=1) # dim * order 2-d tensor\n",
    "        return res\n",
    "        \n",
    "    def integrand(self, x):\n",
    "        \"\"\"\n",
    "        evaluate the integrand function\n",
    "        \n",
    "        :params[in], x, a one-dimensional tensor \n",
    "        \n",
    "        :params[out], res, a real number\n",
    "        \"\"\"\n",
    "        pow_mat = self.vect_power_matrix(x) # power matrix\n",
    "        hadam = self.Alpha * pow_mat  # hadamard product\n",
    "        res = torch.sum(hadam, dim=1).sum().item() # evaluation\n",
    "        return res\n",
    "    \n",
    "    def integral(self):\n",
    "        \"\"\"\n",
    "        compute the real value of the integral\n",
    "        \n",
    "        :params[out], res, a real number\n",
    "        \"\"\"\n",
    "         # values of right part of Eq. 10\n",
    "        vals = [(self.sigma**i)*special.factorial2(i-1).item() if i%2==0 \\\n",
    "                else 0 for i in range(0, self.order)]\n",
    "        prod_mat_vec = torch.matmul(self.Alpha, torch.tensor(vals))\n",
    "        res = prod_mat_vec.sum().item()\n",
    "        return res\n",
    "    \n",
    "    def score_func(self, x):\n",
    "        \"\"\"\n",
    "        evaluate the score function, i.e., gradient of log density\n",
    "        \n",
    "        :params[in], x, a one-dimensional tensor \n",
    "        \n",
    "        :params[out], score, a 1-d tensor\n",
    "        \"\"\"\n",
    "        score = -1*x/(self.sigma**2)\n",
    "        return score\n",
    "    \n",
    "    def sample(self, n):\n",
    "        \"\"\"\n",
    "        draw a sample from the multivariate Gaussian distr.\n",
    "        \n",
    "        :params[in], n, an integer, sample size\n",
    "        \n",
    "        :params[out], samp, a n-by-d matrix tensor, of which each row represents a sample\n",
    "        \"\"\"\n",
    "        samp = self.sigma*torch.randn(n, self.dim)\n",
    "        return samp\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000, 2])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.zeros(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### high-dimensional polynomial integrand with respect to normal distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = 10\n",
    "t1 = [[1., -1.] for i in range(dim)]\n",
    "Alpha, sigma = torch.tensor(t1), 1. # setup\n",
    "prob1 = Prepare_Poly(Alpha, sigma)  # problem 1\n",
    "D_in = Alpha.size()[0]              # dimension of x\n",
    "# simulate data\n",
    "size = 10000  # sample size\n",
    "result_epochs = [3, 5, 10, 15, 20] # the epochs at which the results are recorded\n",
    "num_epochs = len(result_epochs)\n",
    "mc_err, cv_err, comp_time = np.zeros(num_epochs),np.zeros(num_epochs),\\\n",
    "    np.zeros(num_epochs)\n",
    "cls_err,cls_full,cls_time = np.zeros(nrep),np.zeros(nrep),np.zeros(nrep)\n",
    "train_perc, valid_perc = 0.9, .1\n",
    "# for i in range(nrep):   ## repeated sampling\n",
    "X = prob1.sample(size)    # draw sample\n",
    "Y = torch.tensor([prob1.integrand(X[i]) for i in range(size)]) # f(x[i])\n",
    "score_matrix = prob1.score_func(X)   ## score matrix for the data\n",
    "integral = prob1.integral()          ## true value of integral"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quadratic Second order Stein Control variates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 -th Epoch, training loss:  1.163469209432602 current validation loss:  1.203547460079193 best val loss: 1.203547460079193\n",
      "1 -th Epoch, training loss:  0.3103211639722188 current validation loss:  0.3297775376353945 best val loss: 0.3297775376353945\n",
      "2 -th Epoch, training loss:  0.07677746645609537 current validation loss:  0.08308441940375737 best val loss: 0.08308441940375737\n",
      "3 -th Epoch, training loss:  0.019481076876322428 current validation loss:  0.021141732164791652 best val loss: 0.021141732164791652\n",
      "4 -th Epoch, training loss:  0.005055023511250814 current validation loss:  0.005539442896842956 best val loss: 0.005539442896842956\n",
      "5 -th Epoch, training loss:  0.0013356804847717285 current validation loss:  0.0014703112500054495 best val loss: 0.0014703112500054495\n",
      "6 -th Epoch, training loss:  0.0003575609525044759 current validation loss:  0.00039496185098375594 best val loss: 0.00039496185098375594\n",
      "7 -th Epoch, training loss:  9.714746475219727e-05 current validation loss:  0.00010707346030644008 best val loss: 0.00010707346030644008\n",
      "8 -th Epoch, training loss:  2.700010935465495e-05 current validation loss:  2.9877611568995883e-05 best val loss: 2.9877611568995883e-05\n",
      "9 -th Epoch, training loss:  9.275436401367188e-06 current validation loss:  1.0150653975350516e-05 best val loss: 1.0150653975350516e-05\n",
      "10 -th Epoch, training loss:  8.743604024251302e-06 current validation loss:  9.500384330749511e-06 best val loss: 9.500384330749511e-06\n",
      "11 -th Epoch, training loss:  8.417606353759765e-06 current validation loss:  9.10372393471854e-06 best val loss: 9.10372393471854e-06\n",
      "12 -th Epoch, training loss:  8.102734883626303e-06 current validation loss:  8.7334258215768e-06 best val loss: 8.7334258215768e-06\n",
      "13 -th Epoch, training loss:  7.934411366780599e-06 current validation loss:  8.51837226322719e-06 best val loss: 8.51837226322719e-06\n",
      "14 -th Epoch, training loss:  7.735888163248698e-06 current validation loss:  8.291465895516531e-06 best val loss: 8.291465895516531e-06\n",
      "15 -th Epoch, training loss:  7.612864176432292e-06 current validation loss:  8.144395692007882e-06 best val loss: 8.144395692007882e-06\n",
      "16 -th Epoch, training loss:  7.488250732421875e-06 current validation loss:  8.008497101919991e-06 best val loss: 8.008497101919991e-06\n",
      "17 -th Epoch, training loss:  7.274627685546875e-06 current validation loss:  7.793307304382324e-06 best val loss: 7.793307304382324e-06\n",
      "18 -th Epoch, training loss:  7.2130362192789716e-06 current validation loss:  7.696713720049177e-06 best val loss: 7.696713720049177e-06\n",
      "19 -th Epoch, training loss:  7.152160008748373e-06 current validation loss:  7.633090019226075e-06 best val loss: 7.633090019226075e-06\n",
      "Monte Carlo Error: [0.04012107849121094, 0.04012107849121094, 0.04012107849121094, 0.04012107849121094, 0.04012107849121094]  Control Variates Error: [0.0011390898397998228, 7.953056267417935e-05, 9.555476054856626e-08, 1.1244842035296188e-07, 5.018711135562626e-08]  Computing Time: [3.2376503944396973, 5.491885662078857, 10.455648183822632, 15.543457984924316, 20.697250843048096]\n",
      "Exact Control Variates Error: 0.0  Exact Control Variates Compute time: 5.22709059715271\n"
     ]
    }
   ],
   "source": [
    "lr, gamma = 2.e-3, .5       ## learning rate, gamma is the learning rate reducing rate\n",
    "cv1 = SteinSecondOrderQuadPolyCV(D_in, score_matrix)       # Quadratic control variate\n",
    "res = cv1.train(X, Y, integral, lr, gamma, clip_value=10.0, train_perc=.3, step_size=10,\\\n",
    "  weight_decay = 0.0, batch_size = 8, result_epochs=result_epochs, norm_init_std=1.e-4)\n",
    "## monte carlo error, control variates error and computing time\n",
    "mc_err, cv_err, comp_time = res[:3]\n",
    "print('Monte Carlo Error:', mc_err, ' Control Variates Error:',cv_err, ' Computing Time:', comp_time)\n",
    "## use classical method --- to find exact solution\n",
    "res1 =cv1.classical_way(X, Y, integral, train_perc=train_perc)\n",
    "cls_err, cls_time = res1[1:3]     ## exact solution and its computing time\n",
    "print('Exact Control Variates Error:',cls_err,' Exact Control Variates Compute time:',cls_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernel Control variate with the kernel defined in Oates et al. 2017 JRSSB paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = 10\n",
    "t1 = [[1., -1.] for i in range(dim)]\n",
    "Alpha, sigma = torch.tensor(t1), 1. # setup\n",
    "prob1 = Prepare_Poly(Alpha, sigma)  # problem 1\n",
    "D_in = Alpha.size()[0]              # dimension of x\n",
    "# simulate data\n",
    "size = 1000  # sample size\n",
    "result_epochs = [3, 5, 10, 15, 20] # the epochs at which the results are recorded\n",
    "num_epochs = len(result_epochs)\n",
    "mc_err, cv_err, comp_time = np.zeros(num_epochs),np.zeros(num_epochs),\\\n",
    "    np.zeros(num_epochs)\n",
    "cls_err,cls_full,cls_time = np.zeros(nrep),np.zeros(nrep),np.zeros(nrep)\n",
    "# for i in range(nrep):   ## repeated sampling\n",
    "X = prob1.sample(size)    # draw sample\n",
    "Y = torch.tensor([prob1.integrand(X[i]) for i in range(size)]) # f(x[i])\n",
    "score_matrix = prob1.score_func(X)   ## score matrix for the data\n",
    "integral = prob1.integral()          ## true value of integral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 -th Epoch, training loss:  2.4137000312805177 current validation loss:  2.3043442754745485 best val loss: 2.3043442754745485\n",
      "1 -th Epoch, training loss:  2.330137397766113 current validation loss:  2.2285582246780398 best val loss: 2.2285582246780398\n",
      "2 -th Epoch, training loss:  2.2449100580215453 current validation loss:  2.1509262647628784 best val loss: 2.1509262647628784\n",
      "3 -th Epoch, training loss:  2.1612113466262817 current validation loss:  2.075430911064148 best val loss: 2.075430911064148\n",
      "4 -th Epoch, training loss:  2.077588797569275 current validation loss:  2.000262734413147 best val loss: 2.000262734413147\n",
      "5 -th Epoch, training loss:  1.993915514945984 current validation loss:  1.9242565221786498 best val loss: 1.9242565221786498\n",
      "6 -th Epoch, training loss:  1.9143200874328614 current validation loss:  1.852272801399231 best val loss: 1.852272801399231\n",
      "7 -th Epoch, training loss:  1.835290244102478 current validation loss:  1.7811649522781372 best val loss: 1.7811649522781372\n",
      "8 -th Epoch, training loss:  1.7601851377487183 current validation loss:  1.7117398715019225 best val loss: 1.7117398715019225\n",
      "9 -th Epoch, training loss:  1.6852563099861144 current validation loss:  1.643797297000885 best val loss: 1.643797297000885\n",
      "10 -th Epoch, training loss:  1.6488129897117614 current validation loss:  1.6106706891059877 best val loss: 1.6106706891059877\n",
      "11 -th Epoch, training loss:  1.6136061882972716 current validation loss:  1.578487859249115 best val loss: 1.578487859249115\n",
      "12 -th Epoch, training loss:  1.5799088435173034 current validation loss:  1.5476606783866882 best val loss: 1.5476606783866882\n",
      "13 -th Epoch, training loss:  1.5469548544883729 current validation loss:  1.5173125023841858 best val loss: 1.5173125023841858\n",
      "14 -th Epoch, training loss:  1.5145218224525452 current validation loss:  1.4876355805397035 best val loss: 1.4876355805397035\n",
      "15 -th Epoch, training loss:  1.4828605208396912 current validation loss:  1.4590880312919616 best val loss: 1.4590880312919616\n",
      "16 -th Epoch, training loss:  1.4519425320625305 current validation loss:  1.4319016585350037 best val loss: 1.4319016585350037\n",
      "17 -th Epoch, training loss:  1.4220615334510802 current validation loss:  1.405630096912384 best val loss: 1.405630096912384\n",
      "18 -th Epoch, training loss:  1.3936816582679747 current validation loss:  1.3803350739479066 best val loss: 1.3803350739479066\n",
      "19 -th Epoch, training loss:  1.3672796359062196 current validation loss:  1.356865276813507 best val loss: 1.356865276813507\n",
      "Monte Carlo Error: 0.04467010498046875  Control Variates Error: [0.11190562411863425, 0.09219415166415246, 0.046688509121537436, 0.02529536193236659, 0.006986360218375509]  Computing Time: [0.4027937650680542, 0.582128643989563, 1.0319846868515015, 1.5061253309249878, 1.9673131704330444]\n",
      "Exact Control Variates Error: -0.009771347045898438  Exact Control Variates Compute time: 0.37290191650390625\n"
     ]
    }
   ],
   "source": [
    "mean_distance = float(euclidean_distances(X).mean())    ## mean distance of all samples in X, each row represents a sample\n",
    "lr, gamma = 2.e-3, .5             ## learning rate, gamma is the learning rate reducing rate\n",
    "coef1, coef2 = 0.1, 1.0         ## coefficients to multiply mean_distance to get the parameters in Oates et al. Kernel;\n",
    "kernel_alpha1, kernel_alpha2 = mean_distance*coef1, mean_distance*coef2\n",
    "train_pct1 = .5       ## percentage of data used for training\n",
    "result_epochs = [3, 5, 10, 15, 20] # the epochs at which the results are recorded\n",
    "num_epochs = len(result_epochs)\n",
    "mc_err, cv_err, comp_time = np.zeros(num_epochs),np.zeros(num_epochs),\\\n",
    "    np.zeros(num_epochs)\n",
    "cls_err,cls_time = np.zeros(nrep),np.zeros(nrep)   ## least square Control variates error\n",
    "\n",
    "## evaluate the gram matrix for the kernel\n",
    "start_time = time.time()\n",
    "ker2= OatesGram(X.data.numpy(), score_matrix.data.numpy(), alpha1=kernel_alpha1, alpha2=kernel_alpha2)\n",
    "gram_matrix = ker2.gram_matrix()\n",
    "gram_time = time.time()-start_time  ## computing time for gram matrix\n",
    "sigma_sq = 0.0     ## sigma square to add on the gram matrix \n",
    "kernel_cv1 = GaussianControlVariate(X, Y, gram_matrix=gram_matrix, gram_time=gram_time, sigma_sq=sigma_sq,\\\n",
    "                                    train_pct=train_pct1)\n",
    "ls_res1=kernel_cv1.classical_way(Y, integral)    ## use least square to find exact solution\n",
    "cv_res1=kernel_cv1.train(Y, integral, lr, gamma, clip_value=dim/10., valid_perc=(1.-train_pct1),\\\n",
    "                         batch_size=4, N_epochs=result_epochs, step_size=10, weight_decay=1.0e-3)\n",
    "## classical least square to train control variates\n",
    "mc_err, cls_err, cls_time = ls_res1[:3]  ## monte carlo error, exact solution and its computing time\n",
    "##  control variates result --- control variates error and computing time\n",
    "cv_err, comp_time = cv_res1[:2]\n",
    "print('Monte Carlo Error:', mc_err, ' Control Variates Error:',cv_err, ' Computing Time:', comp_time)\n",
    "## use classical method --- to find exact solution\n",
    "print('Exact Control Variates Error:',cls_err,' Exact Control Variates Compute time:',cls_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### comments: the performance of kernel control variates is very sensitive to the choice of hyper-parameters (kernel_alpha1, kernel_alpha2, determined by coef1, and coef2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Networks Control variates "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = 10\n",
    "t1 = [[1., -1.] for i in range(dim)]\n",
    "Alpha, sigma = torch.tensor(t1), 1. # setup\n",
    "prob1 = Prepare_Poly(Alpha, sigma)  # problem 1\n",
    "D_in = Alpha.size()[0]              # dimension of x, i.e., input\n",
    "# simulate data\n",
    "size = 1000  # sample size\n",
    "result_epochs = [3, 5, 10, 15, 20] # the epochs at which the results are recorded\n",
    "num_epochs = len(result_epochs)\n",
    "cv_err, comp_time = np.zeros(num_epochs),np.zeros(num_epochs)\n",
    "\n",
    "# for i in range(nrep):   ## repeated sampling\n",
    "X = prob1.sample(size)    # draw sample\n",
    "Y = torch.tensor([prob1.integrand(X[i]) for i in range(size)]) # f(x[i])\n",
    "score_matrix = prob1.score_func(X)   ## score matrix for the data\n",
    "integral = prob1.integral()          ## true value of integral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def count_parameters(model):\n",
    "    \"\"\"\n",
    "    count the number of training parameters that require gradient\n",
    "    \n",
    "    :params[in]: model, a torch model that has parameters method\n",
    "    \n",
    "    :params[out]: total number of parameters\n",
    "    \"\"\"\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 -th Epoch, training loss:  2.5397046327590944 validation loss:  2.5727998466491697 indicator: 0.000353968063776847\n",
      "1 -th Epoch, training loss:  2.5194113245010374 validation loss:  2.5522395210266113 indicator: 0.000813267309218645\n",
      "2 -th Epoch, training loss:  2.488999274253845 validation loss:  2.5214228725433347 indicator: 0.0014959034211933613\n",
      "3 -th Epoch, training loss:  2.439742917060852 validation loss:  2.4715623817443846 indicator: 0.0025470267517957836\n",
      "4 -th Epoch, training loss:  2.342993344306946 validation loss:  2.37383166885376 indicator: 0.004434479166287929\n",
      "5 -th Epoch, training loss:  2.146729588508606 validation loss:  2.175960771560669 indicator: 0.007917857009917498\n",
      "6 -th Epoch, training loss:  1.7766536855697632 validation loss:  1.8036571350097657 indicator: 0.01346542956493795\n",
      "7 -th Epoch, training loss:  1.212803241252899 validation loss:  1.2357189159393311 indicator: 0.02138813330233097\n",
      "8 -th Epoch, training loss:  0.7240641634464264 validation loss:  0.7425383906364441 indicator: 0.02935880344733596\n",
      "9 -th Epoch, training loss:  0.5387558777332306 validation loss:  0.5544259295463562 indicator: 0.03373339787870645\n",
      "10 -th Epoch, training loss:  0.5276626837253571 validation loss:  0.5426286063194274 indicator: 0.03424412926658988\n",
      "11 -th Epoch, training loss:  0.4783544065952301 validation loss:  0.49291482973098755 indicator: 0.03551548972353339\n",
      "12 -th Epoch, training loss:  0.521640207529068 validation loss:  0.5354176325798035 indicator: 0.035211863201111554\n",
      "13 -th Epoch, training loss:  0.5483418323993683 validation loss:  0.5613472275733947 indicator: 0.03359280550479889\n",
      "14 -th Epoch, training loss:  0.5392366745471955 validation loss:  0.5522725052833557 indicator: 0.03338890977203846\n",
      "15 -th Epoch, training loss:  0.5145664446353913 validation loss:  0.5272834706306457 indicator: 0.034555963069200515\n",
      "16 -th Epoch, training loss:  0.4931369240283966 validation loss:  0.505425733089447 indicator: 0.034873639222234486\n",
      "17 -th Epoch, training loss:  0.4707925627231598 validation loss:  0.4824922490119934 indicator: 0.03510023032873869\n",
      "18 -th Epoch, training loss:  0.4638265058994293 validation loss:  0.47508004236221313 indicator: 0.034980651944875714\n",
      "19 -th Epoch, training loss:  0.45907000374794005 validation loss:  0.4702905993461609 indicator: 0.035642113082110884\n"
     ]
    }
   ],
   "source": [
    "drop_prob, lr, train_pct = 0.5, 1.e-4, .5    ## drop-out probability, learning rate, percent of data for training \n",
    "w_decay, gamma = 5.e-5,.5           ## weight decay, learning rate decay rate\n",
    "## this is the input for neural networks control variates, requires gradients\n",
    "nn_X = X.clone().detach().requires_grad_(True)\n",
    "## control variates with neural nets -- h_dims is the number of neurons in each layer\n",
    "nn_cv1 = SteinFirstOrderNeuralNets(D_in, h_dims=[20]*5, score_matrix=score_matrix,\\\n",
    "                                   init_val=Y.mean(), drop_prob=drop_prob)\n",
    "## split the data into two parts, use Mean squared error to train\n",
    "nn_res1=nn_cv1.trainer(nn_X, Y, integral, lr, gamma, clip_value=.1*count_parameters(nn_cv1),\\\n",
    "                       train_perc=train_pct, valid_perc=(1.-train_pct), batch_size =8,\\\n",
    "                       weight_decay = w_decay, result_epochs=result_epochs)\n",
    "mc_err=Y.mean().item() - integral  ## monte carlo error\n",
    "cv_err, comp_time = nn_res1[1:3]     ## control variates error/ and computing time over epochs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.034076690673828125"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mc_err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.05635274417325853,\n",
       " 0.05341416842816393,\n",
       " 0.02411524971574508,\n",
       " 0.024459737822413175,\n",
       " 0.02220653451234078]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_err"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### comment: Tuning hyper-parameters like learning rate and weight decay rate can improve the performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
